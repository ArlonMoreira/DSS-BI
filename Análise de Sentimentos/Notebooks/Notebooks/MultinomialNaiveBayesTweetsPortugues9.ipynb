{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo didático de análise de sentimentos a partir de uma base de dados de tweets classificados classificados como positivo, negativo e neutro. Baseado no trabalho do Minerando Dados (https://github.com/minerandodados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   8199\n",
       "Created At                   8199\n",
       "Text                         8199\n",
       "Geo Coordinates.latitude      104\n",
       "Geo Coordinates.longitude     104\n",
       "User Location                5489\n",
       "Username                     8199\n",
       "User Screen Name             8199\n",
       "Retweet Count                8199\n",
       "Classificacao                8199\n",
       "Observação                      1\n",
       "Unnamed: 10                     0\n",
       "Unnamed: 11                     0\n",
       "Unnamed: 12                     0\n",
       "Unnamed: 13                     0\n",
       "Unnamed: 14                     0\n",
       "Unnamed: 15                     0\n",
       "Unnamed: 16                     0\n",
       "Unnamed: 17                     0\n",
       "Unnamed: 18                     0\n",
       "Unnamed: 19                     0\n",
       "Unnamed: 20                     0\n",
       "Unnamed: 21                     0\n",
       "Unnamed: 22                     0\n",
       "Unnamed: 23                     0\n",
       "Unnamed: 24                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeiro, vamos contar a quantidade total de registros\n",
    "dataset = pd.read_csv(r'/Users/nadia/Documents/Especialização/SA/Datasets/Tweets_Mg.csv',encoding='utf-8')\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   2453\n",
       "Created At                   2453\n",
       "Text                         2453\n",
       "Geo Coordinates.latitude      102\n",
       "Geo Coordinates.longitude     102\n",
       "User Location                1712\n",
       "Username                     2453\n",
       "User Screen Name             2453\n",
       "Retweet Count                2453\n",
       "Classificacao                2453\n",
       "Observação                      0\n",
       "Unnamed: 10                     0\n",
       "Unnamed: 11                     0\n",
       "Unnamed: 12                     0\n",
       "Unnamed: 13                     0\n",
       "Unnamed: 14                     0\n",
       "Unnamed: 15                     0\n",
       "Unnamed: 16                     0\n",
       "Unnamed: 17                     0\n",
       "Unnamed: 18                     0\n",
       "Unnamed: 19                     0\n",
       "Unnamed: 20                     0\n",
       "Unnamed: 21                     0\n",
       "Unnamed: 22                     0\n",
       "Unnamed: 23                     0\n",
       "Unnamed: 24                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora, apenas os classificados como neutro\n",
    "dataset[dataset.Classificacao == 'Neutro'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   3300\n",
       "Created At                   3300\n",
       "Text                         3300\n",
       "Geo Coordinates.latitude        1\n",
       "Geo Coordinates.longitude       1\n",
       "User Location                2118\n",
       "Username                     3300\n",
       "User Screen Name             3300\n",
       "Retweet Count                3300\n",
       "Classificacao                3300\n",
       "Observação                      1\n",
       "Unnamed: 10                     0\n",
       "Unnamed: 11                     0\n",
       "Unnamed: 12                     0\n",
       "Unnamed: 13                     0\n",
       "Unnamed: 14                     0\n",
       "Unnamed: 15                     0\n",
       "Unnamed: 16                     0\n",
       "Unnamed: 17                     0\n",
       "Unnamed: 18                     0\n",
       "Unnamed: 19                     0\n",
       "Unnamed: 20                     0\n",
       "Unnamed: 21                     0\n",
       "Unnamed: 22                     0\n",
       "Unnamed: 23                     0\n",
       "Unnamed: 24                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Os classificados como positivo\n",
    "dataset[dataset.Classificacao == 'Positivo'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   2446\n",
       "Created At                   2446\n",
       "Text                         2446\n",
       "Geo Coordinates.latitude        1\n",
       "Geo Coordinates.longitude       1\n",
       "User Location                1659\n",
       "Username                     2446\n",
       "User Screen Name             2446\n",
       "Retweet Count                2446\n",
       "Classificacao                2446\n",
       "Observação                      0\n",
       "Unnamed: 10                     0\n",
       "Unnamed: 11                     0\n",
       "Unnamed: 12                     0\n",
       "Unnamed: 13                     0\n",
       "Unnamed: 14                     0\n",
       "Unnamed: 15                     0\n",
       "Unnamed: 16                     0\n",
       "Unnamed: 17                     0\n",
       "Unnamed: 18                     0\n",
       "Unnamed: 19                     0\n",
       "Unnamed: 20                     0\n",
       "Unnamed: 21                     0\n",
       "Unnamed: 22                     0\n",
       "Unnamed: 23                     0\n",
       "Unnamed: 24                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E finalmente, os classificados como negativo\n",
    "dataset[dataset.Classificacao == 'Negativo'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geo Coordinates.latitude</th>\n",
       "      <th>Geo Coordinates.longitude</th>\n",
       "      <th>User Location</th>\n",
       "      <th>Username</th>\n",
       "      <th>User Screen Name</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jan 08 01:22:05 +0000 2017</td>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Leonardo C Schneider</td>\n",
       "      <td>LeoCSchneider</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jan 08 01:49:01 +0000 2017</td>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jan 08 01:01:46 +0000 2017</td>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Wed Jan 04 21:43:51 +0000 2017</td>\n",
       "      <td>��� https://t.co/BnDsO34qK0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ana estudando</td>\n",
       "      <td>estudandoconcur</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon Jan 09 15:08:21 +0000 2017</td>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Milly777</td>\n",
       "      <td>0</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      Created At  \\\n",
       "0           0  Sun Jan 08 01:22:05 +0000 2017   \n",
       "1           1  Sun Jan 08 01:49:01 +0000 2017   \n",
       "2           2  Sun Jan 08 01:01:46 +0000 2017   \n",
       "3           3  Wed Jan 04 21:43:51 +0000 2017   \n",
       "4           4  Mon Jan 09 15:08:21 +0000 2017   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ���⛪ @ Catedral de Santo Antônio - Governador ...   \n",
       "1  � @ Governador Valadares, Minas Gerais https:/...   \n",
       "2  �� @ Governador Valadares, Minas Gerais https:...   \n",
       "3                        ��� https://t.co/BnDsO34qK0   \n",
       "4  ��� PSOL vai questionar aumento de vereadores ...   \n",
       "\n",
       "   Geo Coordinates.latitude  Geo Coordinates.longitude User Location  \\\n",
       "0                       NaN                        NaN        Brasil   \n",
       "1                  -41.9333                     -18.85           NaN   \n",
       "2                  -41.9333                     -18.85           NaN   \n",
       "3                       NaN                        NaN           NaN   \n",
       "4                       NaN                        NaN           NaN   \n",
       "\n",
       "               Username User Screen Name  Retweet Count Classificacao  \\\n",
       "0  Leonardo C Schneider    LeoCSchneider              0        Neutro   \n",
       "1               Wândell         klefnews              0        Neutro   \n",
       "2               Wândell         klefnews              0        Neutro   \n",
       "3         Ana estudando  estudandoconcur              0        Neutro   \n",
       "4                 Emily         Milly777              0      Negativo   \n",
       "\n",
       "      ...      Unnamed: 15  Unnamed: 16  Unnamed: 17  Unnamed: 18  \\\n",
       "0     ...              NaN          NaN          NaN          NaN   \n",
       "1     ...              NaN          NaN          NaN          NaN   \n",
       "2     ...              NaN          NaN          NaN          NaN   \n",
       "3     ...              NaN          NaN          NaN          NaN   \n",
       "4     ...              NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 19  Unnamed: 20  Unnamed: 21  Unnamed: 22  Unnamed: 23  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 24  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK, vamos dar uma olhada rápida no conteúdo do dataset para finalizar esse passo\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo o Modelo¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '���⛪ @ Catedral de Santo Antônio - Governador Valadares/MG https://t.co/JSbKamIqUJ',\n",
       "       '� @ Governador Valadares, Minas Gerais https://t.co/B3ThIDJCSf',\n",
       "       '�� @ Governador Valadares, Minas Gerais https://t.co/dPkgzVR2Qw',\n",
       "       ...,\n",
       "       'Trio é preso suspeito de roubo, tráfico e abuso sexual em Uberlândia https://t.co/zaQbXRRJWc',\n",
       "       'Trio é preso suspeito de roubo, tráfico e abuso sexual em Uberlândia: Um dos autores teria molestado vítima de… https://t.co/lQ8cTSNftA',\n",
       "       'Trio suspeito de roubo de cargas é preso em Santa Luzia (MG) https://t.co/0INgJcMtZb #R7MG #RecordTVMinas'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Próximo passo, vamos separar os tweets e suas classes\n",
    "tweets = dataset[\"Text\"].values\n",
    "tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutro', 'Neutro', 'Neutro', ..., 'Positivo', 'Positivo',\n",
       "       'Positivo'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = dataset[\"Classificacao\"].values\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora, vamos treinar o modelo usando a abordagem Bag of Words e o algoritmo Naive Bayes Multinomial\n",
    "#    - Bag of Words, na prática, cria um vetor com cada uma das palavras do texto completo da base,\n",
    "#      depois, calcula a frequência em que essas palavras ocorrem em uma data sentença, para então\n",
    "#      classificar/treinar o modelo\n",
    "#    - Exemplo HIPOTÉTICO de três sentenças vetorizadas \"por palavra\" e classificadas baseada na\n",
    "#      frequência de suas palavras:\n",
    "#         {0,3,2,0,0,1,0,0,0,1, Positivo}\n",
    "#         {0,0,1,0,0,1,0,1,0,0, Negativo}\n",
    "#         {0,1,1,0,0,1,0,0,0,0, Neutro}\n",
    "#    - Olhando para esses vetores, meu palpite é que as palavras nas posições 2 e 3 são as com maior\n",
    "#      peso na determinação de a que classe pertence cada uma das três sentenças avaliadas\n",
    "#    - A função fit_transform faz exatamente esse processo: ajusta o modelo, aprende o vocabulário,\n",
    "#      e transforma os dados de treinamento em feature vectors, a.k.a. vetor com frequêcia das palavras\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "freq_tweets = vectorizer.fit_transform(tweets)\n",
    "\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_tweets, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutro', 'Neutro', 'Negativo', 'Negativo', 'Neutro', 'Positivo'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos usar algumas frases de teste para fazer a classificação com o modelo treinado\n",
    "testes = [\"Esse governo está no início, vamos ver o que vai dar\",\n",
    "          \"Estou muito feliz com o governo de São Paulo esse ano\",\n",
    "          \"O estado de Minas Gerais decretou calamidade financeira!!!\",\n",
    "          \"A segurança desse país está deixando a desejar\",\n",
    "          \"O governador de Minas é do PT\",\n",
    "          \"O prefeito de São Paulo está fazendo um ótimo trabalho\"]\n",
    "\n",
    "freq_testes = vectorizer.transform(testes)\n",
    "modelo.predict(freq_testes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutro', 'Neutro', 'Neutro', ..., 'Positivo', 'Positivo',\n",
       "       'Positivo'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validação cruzada do modelo. Neste caso, o modelo é dividido em 10 partes, treinado em 9 e testado em 1\n",
    "resultados = cross_val_predict(modelo, freq_tweets, classes, cv = 10)\n",
    "resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8831564824978656"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quão acurada é a média do modelo?\n",
    "metrics.accuracy_score(classes, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Positivo       0.95      0.88      0.91      3300\n",
      "   Negativo       0.89      0.93      0.91      2446\n",
      "     Neutro       0.80      0.84      0.82      2453\n",
      "\n",
      "avg / total       0.89      0.88      0.88      8199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Medidas de validação do modelo\n",
    "sentimentos = [\"Positivo\", \"Negativo\", \"Neutro\"]\n",
    "print(metrics.classification_report(classes, resultados, sentimentos))\n",
    "\n",
    "# Lembrando que:\n",
    "#    : precision = true positive / (true positive + false positive)\n",
    "#    : recall    = true positive / (true positive + false negative)\n",
    "#    : f1-score  = 2 * ((precision * recall) / (precision + recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito   Negativo  Neutro  Positivo   All\n",
      "Real                                      \n",
      "Negativo      2275     162         9  2446\n",
      "Neutro         240    2067       146  2453\n",
      "Positivo        45     356      2899  3300\n",
      "All           2560    2585      3054  8199\n"
     ]
    }
   ],
   "source": [
    "# Vamos fazer uma matriz de confusão -- What?!?!\n",
    "print(pd.crosstab(classes, resultados, rownames = [\"Real\"], colnames=[\"Predito\"], margins=True))\n",
    "\n",
    "# Lembrando que:\n",
    "#    - Predito = O que o programa classificou como Negativo, Neutro, Positivo e All\n",
    "#    - Real    = O que é de fato Negativo, Neutro, Positivo e All\n",
    "#\n",
    "# Ou seja, somente 9 tweets eram de fato negativos e o programa classificou como positivos. Já os\n",
    "# positivos que o programa classificou como negativos foram 45, muito mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melhorando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Com o modelo de Bigrams, em lugar de vetorizar o texto \"por palavra\", vamos vetoriza-lo por cada\n",
    "# \"duas palavras\", tipo: Eu gosto de São Paulo => { eu gosto, gosto de, de são, são paulo }\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 2))\n",
    "freq_tweets = vectorizer.fit_transform(tweets)\n",
    "\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_tweets, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutro', 'Neutro', 'Neutro', ..., 'Positivo', 'Positivo',\n",
       "       'Positivo'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nova predição bigramada\n",
    "resultados = cross_val_predict(modelo, freq_tweets, classes, cv = 10)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89547505793389437"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qual foi a acuracidade desse novo modelo?\n",
    "metrics.accuracy_score(classes, resultados)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Positivo       0.97      0.88      0.92      3300\n",
      "   Negativo       0.91      0.93      0.92      2446\n",
      "     Neutro       0.80      0.89      0.84      2453\n",
      "\n",
      "avg / total       0.90      0.90      0.90      8199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As novas medidas de validação do modelo, um pouquinho melhor que o anterior\n",
    "print(metrics.classification_report(classes, resultados, sentimentos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito   Negativo  Neutro  Positivo   All\n",
      "Real                                      \n",
      "Negativo      2265     179         2  2446\n",
      "Neutro         181    2177        95  2453\n",
      "Positivo        43     357      2900  3300\n",
      "All           2489    2713      2997  8199\n"
     ]
    }
   ],
   "source": [
    "# E a nova matriz de confusão\n",
    "print(pd.crosstab(classes, resultados, rownames = [\"Real\"], colnames = [\"Predito\"], margins = True))\n",
    "\n",
    "# Mudanças em relação ao modelo anterior:\n",
    "#\n",
    "#    - Negativo-negativo = 2275 vs 2265 (piorou)\n",
    "#    - Negativo-neutro   = 162  vs 179  (piorou)\n",
    "#    - Negativo-positivo = 9    vs 2    (melhorou)\n",
    "#\n",
    "#    - Positivo-positivo = 2899 vs 2900 (melhorou)\n",
    "#    - Positivo-neutro   = 356  vs 357  (piorou)\n",
    "#    - Positivo-negativo = 45   vs 43   (melhorou)\n",
    "#\n",
    "#    - Neutro-neutro     = 2067 vs 2177 (melhorou)\n",
    "#    - Neutro-negativo   = 240  vs 181  (melhorou)\n",
    "#    - Neutro-positivo   = 146  vs 95   (melhorou)\n",
    "#\n",
    "# Tabela anterior para referência:\n",
    "#\n",
    "#    Predito   Negativo  Neutro  Positivo   All\n",
    "#    Real                                      \n",
    "#    Negativo      2275     162         9  2446\n",
    "#    Neutro         240    2067       146  2453\n",
    "#    Positivo        45     356      2899  3300\n",
    "#    All           2560    2585      3054  8199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melhorando o modelo ---> BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8199x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 120891 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos reinicializar nosso bag of words com um parâmetro de máximo de features\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                             stop_words = None, max_features = 5000)\n",
    "\n",
    "# Treinar o modelo, aprender o vocabulário e transformar nossos dados de treinamento em feature vectors\n",
    "train_data_features = vectorizer.fit_transform(tweets)\n",
    "train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hora de iniciar um classificador Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutro', 'Neutro', 'Neutro', ..., 'Positivo', 'Positivo',\n",
       "       'Positivo'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar os sentimentos do dataset de tweets\n",
    "class_sentimentos = dataset[\"Classificacao\"].values\n",
    "class_sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajusta a forest ao dataset de treinamento usando a bag of words como feature e os sentimentos\n",
    "# como a resposta variável\n",
    "forest = forest.fit(train_data_features, class_sentimentos)\n",
    "forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 45 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar a bag of words de teste\n",
    "test_data_features = vectorizer.transform(testes)\n",
    "test_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutro', 'Neutro', 'Neutro', 'Neutro', 'Neutro', 'Neutro'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazer um predição\n",
    "resultados = forest.predict(test_data_features)\n",
    "resultados\n",
    "\n",
    "# Resultado que tivemos com o primeiro modelo:\n",
    "# array(['Neutro', 'Neutro', 'Negativo', 'Negativo', 'Neutro', 'Positivo'], dtype='<U8')\n",
    "#\n",
    "# Meh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Esse governo está no início, vamos ver o que v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Estou muito feliz com o governo de São Paulo e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>O estado de Minas Gerais decretou calamidade f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>A segurança desse país está deixando a desejar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>O governador de Minas é do PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>O prefeito de São Paulo está fazendo um ótimo ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id sentimento                                              texto\n",
       "0   1     Neutro  Esse governo está no início, vamos ver o que v...\n",
       "1   2     Neutro  Estou muito feliz com o governo de São Paulo e...\n",
       "2   3     Neutro  O estado de Minas Gerais decretou calamidade f...\n",
       "3   4     Neutro     A segurança desse país está deixando a desejar\n",
       "4   5     Neutro                      O governador de Minas é do PT\n",
       "5   6     Neutro  O prefeito de São Paulo está fazendo um ótimo ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Que tal gerar uma tabelinha Pandas?\n",
    "testes_id = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "data_frame = pd.DataFrame(data = { \"id\": testes_id, \"texto\": testes, \"sentimento\": resultados })\n",
    "data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11048\n",
      "drwxr-xr-x  14 nadia  staff      448 Feb 28 21:55 ALLProjects\n",
      "drwxr-xr-x   7 nadia  staff      224 Mar  1 11:40 Datasets\n",
      "drwxr-xr-x   9 nadia  staff      288 Feb 28 11:02 Léxicos_Portugues\n",
      "drwxr-xr-x  23 nadia  staff      736 Mar  1 11:59 Notebooks\n",
      "-rw-------@  1 nadia  staff  2674277 Feb 20 15:54 aula1.pptx\n",
      "-rw-r--r--@  1 nadia  staff  2367718 Feb 27 20:58 aula2.pptx\n",
      "drwxr-xr-x@  3 nadia  staff       96 Feb  2 20:47 outros\n",
      "-rw-r--r--@  1 nadia  staff   593750 Feb  2 20:22 paperFacebook.pdf\n",
      "-rwxr-xr-x@  1 nadia  staff     3029 Jan  2 11:56 sentiment_analysis_supervised_ml.py\n",
      "-rw-------@  1 nadia  staff      165 Jan 27 13:06 ~$aula1.potx\n",
      "-rw-------@  1 nadia  staff      165 Jan 27 14:33 ~$aula1.pptx\n",
      "-rw-r--r--@  1 nadia  staff      165 Feb 28 09:33 ~$aula2.pptx\n",
      "\n",
      "total 137504\n",
      "-rw-r--r--  1 nadia  staff      5166 Feb 27 22:50 Afinn_sentiment_analysis_unsupervised_lexical_Movie_reviews4.ipynb\n",
      "-rw-r--r--  1 nadia  staff    339480 Feb 28 22:48 ColetaTwitterSentPortgues7.ipynb\n",
      "-rw-r--r--@ 1 nadia  staff    369379 Feb 28 11:02 LIWC-original.txt\n",
      "-rw-r--r--  1 nadia  staff      4330 Mar  1 10:44 LIWCSentimentAnalysis6.ipynb\n",
      "-rw-r--r--  1 nadia  staff    228781 Mar  1 11:29 LIWCSentimentAnalysisCorpusBuscaPe8.ipynb\n",
      "-rw-r--r--  1 nadia  staff      1684 Feb 28 09:43 NLTK SentimentAnalyzer.ipynb\n",
      "-rw-r--r--  1 nadia  staff      9431 Feb 27 22:33 SentiWordNet_ExampleComplete2.ipynb\n",
      "-rw-r--r--  1 nadia  staff     15411 Feb 28 09:52 SentiWordNet_Sentiment_analysis_unsupervised_lexical_Movie_reviews3.ipynb\n",
      "-rw-r--r--@ 1 nadia  staff     50251 Feb 27 22:25 SentiWordNet_SimplesUso1.ipynb\n",
      "-rw-r--r--  1 nadia  staff       555 Feb 28 22:10 Untitled.ipynb\n",
      "-rw-r--r--  1 nadia  staff     33254 Mar  1 11:58 Untitled1.ipynb\n",
      "-rw-r--r--  1 nadia  staff      5477 Feb 28 10:14 VaderSentimentAnalyser5.ipynb\n",
      "drwxr-xr-x  3 nadia  staff        96 Feb 27 23:18 __pycache__\n",
      "-rwxr-xr-x@ 1 nadia  staff      3052 Jan  2 11:56 contractions.py\n",
      "-rw-r--r--@ 1 nadia  staff   2102624 Feb 28 11:50 corpusBuscape.sql\n",
      "-rw-r--r--@ 1 nadia  staff  66262310 Feb 27 21:33 movie_reviews.csv\n",
      "-rwxr-xr-x@ 1 nadia  staff      5602 Jan  2 11:56 normalization.py\n",
      "-rwxr-xr-x@ 1 nadia  staff     11535 Jan  2 11:56 sentiment_analysis_unsupervised_lexical.py\n",
      "-rw-r--r--  1 nadia  staff       383 Mar  1 12:00 tweets_classificados_por_forest.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# E finalmente, vamos salvar nossa predição em um .csv\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"-l\", \"../\"]).decode(\"utf8\")) #O comando ls -l exibe o conteúdo da pasta em forma de lista:\n",
    "\n",
    "\n",
    "data_frame.to_csv(\"tweets_classificados_por_forest.csv\", index = False, quoting = 3, escapechar = \"\\\\\")\n",
    "print(check_output([\"ls\", \"-l\", \".\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,sentimento,texto\n",
      "1,Neutro,Esse governo está no início\\, vamos ver o que vai dar\n",
      "2,Neutro,Estou muito feliz com o governo de São Paulo esse ano\n",
      "3,Neutro,O estado de Minas Gerais decretou calamidade financeira!!!\n",
      "4,Neutro,A segurança desse país está deixando a desejar\n",
      "5,Neutro,O governador de Minas é do PT\n",
      "6,Neutro,O prefeito de São Paulo está fazendo um ótimo trabalho\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OK, ok, vamos dar só mais uma espiada para ver se deu tudo certo\n",
    "print(check_output([\"cat\", \"tweets_classificados_por_forest.csv\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
